# ML Model Inference with FastAPI

## ğŸ“Œ Project Description
This project demonstrates how to deploy a Machine Learning model using **FastAPI** for real-time inference. Users can send input data to the API and get predictions instantly.

## ğŸš€ Features
- Fast and lightweight API
- Easy integration with trained ML models
- Interactive Swagger UI available at `/docs` endpoint
- Example requests included for testing

## ğŸ› ï¸ Technologies Used
- Python
- FastAPI
- Uvicorn
- Scikit-learn / Pandas (for ML model)
- Git & GitHub (for version control)

## âš™ï¸ How to Run Locally
1. Clone this repository:  
   ```bash
   git clone https://github.com/kawyasathsarani/ML-Model-Inference-with-FastAPI.git

2.Navigate to the project folder:
    
    cd ML-Model-Inference-with-FastAPI

3.Install dependencies:
    
    pip install -r requirements.txt

4.Run the FastAPI app:
    
    uvicorn main:app --reload

5.Open your browser and go to:
    
    http://127.0.0.1:8000/docs

ğŸ“‚ Project Structure

ML-Model-Inference-with-FastAPI/
â”‚
â”œâ”€ main.py             # FastAPI app
â”œâ”€ model.pkl           # Trained ML model
â”œâ”€ requirements.txt    # Python dependencies
â”œâ”€ README.md           # Project description


ğŸ’¡Send a POST request to /predict with JSON data like:

{
  "sepal_length": 5.1,
  "sepal_width": 3.5,
  "petal_length": 1.4,
  "petal_width": 0.2
}


ğŸ‘©â€ğŸ’» Author
Kawya Sathsarani


ğŸ“„ License

### **To update your README on GitHub:**
1. Save this text as `README.md` in your project folder.  
2. Run in terminal:  
```bash
git add README.md
git commit -m "Update README with license link and full details"
git push
